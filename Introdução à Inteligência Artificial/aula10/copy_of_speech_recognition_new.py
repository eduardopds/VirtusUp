# -*- coding: utf-8 -*-
"""Copy of Speech_Recognition-new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ul6FQpFnu-FvjXUDvx38PkpCPVXEot3G

# Reconecimento de fala 

Notebook baseado neste artigo [aqui.](https://www.analyticsvidhya.com/blog/2019/07/learn-build-first-speech-to-text-model-python/)

O objetivo é reconhecer 4 comandos básicos (yes, no, on, off) utilizando uma rede neural convolucional 1D. 

Vamos começar obtendo um arquivo simplificado de comandos de voz (no formato wav, organizados em pastas, que é disponibilizado por Google (parte dos dados para teste no ambiente TensorFlow para implementação de modelos *Deep Learning*). O arquivo contém muito mais comandos do que precisamos. 
## Obtendo e descompactando os arquivos de audio
"""

#DADOS='data'
DADOS='data2'
import urllib.request
#urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=13_x2g79Mx-aXWaYPrprXjkhZaEwv1lJh', 'data.zip')
urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1a8u-PFsEcz5eq5LCfA2X1ymRy6DYJPo5','data2.zip')

!unzip  -n $DADOS".zip"

"""## Importando as bibliotecas necessárias"""

import os
import librosa
import IPython.display as ipd
import matplotlib.pyplot as plt
import numpy as np
from scipy.io import wavfile
import warnings

warnings.filterwarnings("ignore")

"""**Explorando e visualizando os dados**

*Sinal de audio no domínio do tempo*. Experimente visualizar exemplos de audio de cada uma das classes.

OBS: Caso mude o dataset para data2 na primeira célula deste notebook, você precisará escolher outros exemplos de audio para exibir aqui. Utilize comandos como %ls para listar o conteúdo da pasta contendo o novo dataset descompactado.
"""

train_audio_path = DADOS+'/train/'
#yes_files = train_audio_path+'yes'
#%ls $yes_files
filename=train_audio_path+'yes/ff21fb59_nohash_0.wav'
#filename = train_audio_path+'off/0ff728b5_nohash_2.wav'
#filename = train_audio_path+'yes/fe291fa9_nohash_0.wav'
samples, sample_rate = librosa.load(filename, sr = 16000)
fig = plt.figure(figsize=(14, 8))
ax1 = fig.add_subplot(211)
ax1.set_title('Raw wave of ' +filename)
ax1.set_xlabel('time')
ax1.set_ylabel('Amplitude')
print('frac',sample_rate/len(samples))
ax1.plot(np.linspace(0, sample_rate/len(samples), sample_rate), samples)

"""*Visualizando o spectograma do sinal*

A função stft (short-time Fourier transform) representa um sinal no domínio da frequência calculando transformações discretas de Fourier (DFT) em pequenas janelas sobrepostas.

"""

import librosa.display
X = librosa.stft(samples)
print(X)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(8, 3))
librosa.display.specshow(Xdb, x_axis='time', y_axis='linear')
plt.colorbar(format='%+2.0f dB')

"""**Taxa de amostragem**

Vejamos qual é a taxa de amostragem utilizada na aquisição dos audios. Como fica a audição de um sinal se alterarmos a taxa de amostragem original para outra,  no método de playback do audio?
"""

print(sample_rate)

ipd.Audio(samples, rate=sample_rate, autoplay=True)

ipd.Audio(samples, rate=sample_rate/4,  autoplay=True)

ipd.Audio(samples, rate=sample_rate*2,  autoplay=True)

"""**Reamostragem**
Pelo exposto, podemos entender que a taxa de amostragem do sinal é de 16000 hz. Vamos reamostrá-lo para 8000 hz, pois a maioria das frequências relacionadas à fala está presente em no máximo 8000z. Você notou alguma mudança na forma como o audio soa? Mudou a qualidade?
"""

samples = librosa.resample(samples, sample_rate, 8000)
ipd.Audio(samples, rate=8000, autoplay=True)

"""Vamos agora obter o número de gravações para cada comando de voz:"""

labels=os.listdir(train_audio_path)
print(labels)

#computa o número de gravações para cada comando e desenha um gráfico de barra
no_of_recordings=[]
for label in labels:
    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]
    no_of_recordings.append(len(waves))
    
#plot
plt.figure(figsize=(12,5))
index = np.arange(len(labels))
plt.bar(index, no_of_recordings)
plt.xlabel('Commands', fontsize=12)
plt.ylabel('No of recordings', fontsize=12)
plt.xticks(index, labels, fontsize=15, rotation=60)
plt.title('No. of recordings for each command')
plt.show()

#labels=["off", "yes", "on", "no"]
print(labels)

"""**Duração das gravações**

Vamos agora computar as distribuições de duração de todas as gravações. O que dá para concluir a partir do gráfico?
"""

duration_of_recordings=[]
for label in labels:
    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]
    for wav in waves:
        sample_rate, samples = wavfile.read(train_audio_path + '/' + label + '/' + wav)
        duration_of_recordings.append(float(len(samples)/sample_rate))
    
plt.hist(np.array(duration_of_recordings))

"""**Pré-processando os arquios de audio**

Na parte de exploração de dados anterior, vimos que a duração de algumas gravações é inferior a 1 segundo e a taxa de amostragem é muito alta. Então, vamos ler os sinais de áudio e usar as etapas de pré-processamento apresentadas a seguir,  para lidar com isso.

Aqui estão os dois passos que seguiremos:

* Reamostragem
* Remoção de comandos com sinal mais curto do que 1 segundo

Vamos definir essas etapas de pré-processamento no snippet de código abaixo:
"""

all_wave = []
all_label = []
for label in labels:
    print(label)
    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]
    for wav in waves:
        samples, sample_rate = librosa.load(train_audio_path + '/' + label + '/' + wav, sr = 16000)
        samples = librosa.resample(samples, sample_rate, 4000)
        if(len(samples)== 4000) : 
            all_wave.append(samples)
            all_label.append(label)

print(len(all_wave))
#print(all_wave)
print(all_label.count('no'))
print(all_label.count('yes'))
print(all_label.count('on'))
print(all_label.count('off'))

"""Convertendo os rótulos de saída (strings) para inteiros."""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y=le.fit_transform(all_label)
classes= list(le.classes_)
print(all_label)
print(y)

"""Agora, converteremos os rótulos codificados com números inteiros em um vetor do tipo one-hot-encoding (ou winner-takes-all), pois é um problema de classificação múltipla e é esse o tipo de entrada esperada por modelos de redes neurais em geral. Neste exemplo, utilizaremos uma rede neural convolucional com convoluções 1D.

"""

from keras.utils import np_utils
y=np_utils.to_categorical(y, num_classes=len(labels))
print(y)

"""Remodele a matriz 2D para 3D, pois a entrada para o conv1d deve ser uma matriz 3D:

"""

print(all_wave)
all_wave = np.array(all_wave).reshape(-1,4000,1)
print(all_wave.shape)
print(all_wave)

"""**Dividir os dados em treinamento e validação**

Treinaremos o modelo em 80% dos dados e validaremos com os 20% restantes:
"""

from sklearn.model_selection import train_test_split
x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)

print(len(x_tr))
print(len(x_val))

"""**Arquitetura de modelo para este problema**

Vamos construir o modelo que converte de fala para texto usando camadas conv1d. Como já mencionado, Conv1d é uma camada de rede neural convolucional que realiza a convolução em apenas uma dimensão.

**Construção do Modelo de Classificação**

Vamos utilizar a interface funcional do Keras para este propósito. O modelo está ilustrado na figura abaixo e o código vem logo após. 
![alt text](https://drive.google.com/uc?export=view&id=16Lsafs24Hqq9Jp_OjBmvpi9wWdW7XvBg)
"""

from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D
from keras.models import Model
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import backend as K
K.clear_session()

inputs = Input(shape=(4000,1))

#First Conv1D layer
conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.3)(conv)

#Second Conv1D layer
conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.3)(conv)

#Third Conv1D layer
conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.3)(conv)

#Fourth Conv1D layer
conv = Conv1D(64, 7, padding='valid', activation='relu', strides=1)(conv)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.3)(conv)

#Flatten layer
conv = Flatten()(conv)

#Dense Layer 1
conv = Dense(256, activation='relu')(conv)
conv = Dropout(0.3)(conv)

#Dense Layer 2
conv = Dense(128, activation='relu')(conv)
conv = Dropout(0.3)(conv)

outputs = Dense(len(labels), activation='softmax')(conv)

model = Model(inputs, outputs)
model.summary()

"""Definimos a função de perda como entropia cruzada categórica, pois é um problema de multi-classificação:



"""

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

""" Interromperemos o treinamento da rede neural no momento certo (parada por validação) e salvaremos o melhor modelo após cada época:"""

# Commented out IPython magic to ensure Python compatibility.
# %rm -rf models
# %mkdir models

es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10, min_delta=0.0001)
mc = ModelCheckpoint('./models/best_model.hdf5', monitor='val_loss', mode='auto', verbose=1,save_best_only=True)

"""Vamos treinar o modelo em um tamanho de lote (batch size) 32 e avaliaremos o seu desempenho no conjunto de validação:

"""

history=model.fit(x_tr, y_tr ,epochs=100, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))

"""**Curva de Aprendizagem**

Avaliaremos a seguir a evolução das curvas de erro de treinamento e de validação (test) ao longo das várias épocas de treinamento. 
"""

# Commented out IPython magic to ensure Python compatibility.
!date
# %ls -l models

from matplotlib import pyplot
pyplot.plot(history.history['loss'], label='train_loss')
pyplot.plot(history.history['val_loss'], label='val_loss')
pyplot.legend()
pyplot.show()

"""**Carregando o melhor modelo para a memória**"""

from keras.models import load_model
model=load_model('./models/best_model.hdf5')

"""Definindo uma função que prediz o texto (classe) para um dado comando de audio:"""

def predict(audio):
    prob=model.predict(audio.reshape(1,4000,1))
    index=np.argmax(prob[0])
    return classes[index]

"""Hora da previsão! Faremos previsões nos dados de validação. Um sinal de audio de validação é sorteado aleatoriamente  a cada chamada."""

import random
index=random.randint(0,len(x_val)-1)
samples=x_val[index].ravel()
print("Audio de entrada:",classes[np.argmax(y_val[index])])
ipd.Audio(samples, rate=4000, autoplay=True)

print("Classe predita:",predict(samples))

"""Calculando a matriz de confusão e as estatísticas de classificação no conjunto de teste

"""

from sklearn.metrics import confusion_matrix, classification_report
y_pred = model.predict(x_val)
y_pred_int = y_pred.argmax(axis=1)
y_test_int = y_val.argmax(axis=1)
print(confusion_matrix(y_test_int, y_pred_int))
print(classification_report(y_test_int, y_pred_int))

"""A melhor parte ainda está por vir! Temos aqui um script que solicita que um usuário grave comandos de voz.
Experimente seus próprios comandos de voz e teste-o no modelo
"""

# all imports
from IPython.display import Javascript
from google.colab import output
from base64 import b64decode

RECORD = """
const sleep  = time => new Promise(resolve => setTimeout(resolve, time))
const b2text = blob => new Promise(resolve => {
  const reader = new FileReader()
  reader.onloadend = e => resolve(e.srcElement.result)
  reader.readAsDataURL(blob)
})
var record = time => new Promise(async resolve => {
  stream = await navigator.mediaDevices.getUserMedia({ audio: true })
  recorder = new MediaRecorder(stream)
  chunks = []
  recorder.ondataavailable = e => chunks.push(e.data)
  recorder.start()
  await sleep(time)
  recorder.onstop = async ()=>{
    blob = new Blob(chunks)
    text = await b2text(blob)
    resolve(text)
  }
  recorder.stop()
})
"""

def record(sec=1):
  display(Javascript(RECORD))
  s = output.eval_js('record(%d)' % (sec*1000))
  b = b64decode(s.split(',')[1])
  with open('audio.wav','wb') as f:
    f.write(b)
  return 'audio.wav'

record(sec=1.2)

"""Vamos agora ler o arquivo de áudio contendo o comando de voz salvo e, em seguida, convertê-lo em texto usando a rede neural treinada nas etapas anteriores. 

"""

os.listdir('.')

filepath='./'

#reading the voice commands
samples, sample_rate = librosa.load(filepath + '/' + 'audio.wav', sr = 16000)
samples = librosa.resample(samples, sample_rate, 4000)
ipd.Audio(samples,rate=4000, autoplay=True)

#converting voice commands to text
predict(samples[:4000])

"""Parabéns! Você acabou de criar seu próprio modelo de conversão de sinais de fala para texto!

## Agora é a sua vez

1. Altere o nome do arquivo de dados (no início deste notebook) de 'data' para 'data2' e veja o que muda nos resultados. Mantenha o arquivo data2 para o restante dos itens abaixo. Melhorou? Piorou? Tente explicar a razão do resultado ter mudado.
2. Qual o efeito no resultado final de classificação se você reduzir a taxa de amostragem para 4kHz?

## Atividade Bônus
1. Experimente reduzir pela metade a quantidade de filtros convolucionais de todas as camadas da arquitetura neural e avalie o efeito no treinamento e avaliação do modelo na taxa original de 8kHz. Apresente uma discussão dos novos resultados comparativamente aos anteriores.

Resposta 1: Melhorou. Podemos observar por meio da matriz de confusão que obtivemos mais acertos percentualmente usando o data2. Essa melhoria pode ser explicada pelo conjunto de treinamento no data2 ser maior que no data, fazendo com que tenhamos uma rede mais coerente e um erro de treinamento ligeiramente menor que erro de teste.

Resposta 2: Após rodar algumas vezes com 8kHz e 4kHz, verifiquei que com 4kHz geralmente obtém uma acurácia melhor do que 8hz. Mas não percebi uma diferença tão significativa, pois em algumas vezes com 8kHz saiu melhor, ou bem próximo.
"""