# -*- coding: utf-8 -*-
"""Copy of Floresta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQHyJz4Nemcbnj37VrufLEVPHXmvr9fz

## Notebook para ilustrar diferenças entre os corpus 'treebank' e 'floresta' e por que não é possível realizar parsing na gramática probabilística induzida de 'floresta'

Ao final deste novebook é proposta uma Atividade 10 complementar.

Instalando a versão mais recente do NLTK (3.5), o default do colab é 3.2. Esta foi uma tentativa de eliminar o erro (instalando a versão mais recente do NLTK), mas não resolveu, pois o problema  reside, na verdade, no corpus 'floresta'.
"""

!pip install --upgrade nltk

"""Fazendo download dos corpus 'floresta' e 'treebank'"""

import nltk
nltk.download('floresta')
nltk.download('treebank')

"""Imprimindo sentenças"""

from nltk.corpus import floresta
from nltk.corpus import treebank

print("Corpus Floresta")
x=floresta.sents()
#imprime as 3 primeiras sentenças 
for i in range(3):
  print(x[i])
#imprime as sentenças de índices 2000 a 2002
for i in range(2000,2003):
  print(x[i])

print("\n")
print("Corpus Treebank")
x=treebank.sents()
#imprime as 3 primeiras sentenças 
for i in range(3):
  print(x[i])
#imprime as sentenças de índices 2000 a 2002
for i in range(2000,2003):
  print(x[i])

"""Cada treebank possui um ou mais arquivos de texto (fileid) de onde as estruturas da linguagem (como sentenças, palavras, árvores de parsing etc) podem ser acessadas.

Além disto, o corpus floresta tem algumas árvores de parsing com problemas. Para evitar mensagens de erro associadas a tais árvores com problema, você pode desabilitá-las  temporariamente usando as funções auxiliares a seguir.
"""

#funções auxiliares para controle de saída de erro padrão
import os
import sys
def disable_stderr():
  r=sys.stderr
  f = open(os.devnull, 'w')
  sys.stderr = f
  return r

def enable_stderr(f):
  sys.stderr=f

#desabilitando std_err. Para habilitar, basta chamar enable_stderr(r)
r=disable_stderr()

"""O corpus treebank tem muitos fileids (199), enquanto que o corpus floresta possui apenas 1."""

#computa e imprime totais de arquivos de texto de cada corpus
lf=len(floresta.fileids())
lt=len(treebank.fileids())

print('len(floresta) ', lf)
print('len(treebank) ', lt)

"""Vamos agora calcular os totais de árvores de parsing de cada corpus de texto.

Ao executar o código abaixo, é possível perceber que os 3 primeiros textos do corpus 'treebank' contem apenas 33 árvores de parsing (que foram usadas para treinar a gramática do notebook original), enquanto que o único texto do corpus 'floresta' possui 9266 árvores! 
"""

#contando total de árvores nos 3 primeiros textos do corpus 'treebank' (apenas estes 3 textos foram utilizados no notebook NLTK-new.ipynb)
acc=0
for i in treebank.fileids()[:3]:
  acc = acc+len(treebank.parsed_sents(i))
print('total_arvores treebank=',acc)


#contando total de árvores no único texto disponível do corpus 'floresta'
acc=0
for i in floresta.fileids()[:1]:
  lf = len(floresta.parsed_sents(i))
  acc = acc+lf
print('total_arvores floresta=',acc)

#habilitando novamente std_err para que as mensagens de erro voltem a ser impressas
enable_stderr(r)

"""Vamos agora inspecionar os totais de palavras de cada corpus. Percebam que o corpus floresta é bem mais rico em número de palavras."""

print("floresta.words=",len(floresta.words()), "\ntreebank.words=", len(treebank.words()))

"""Vamos agora aprender a percorrer as árvores de parsing do corpus 'floresta' e normalizar as regras de produção, evitando aquelas que não podem ser normalizadas.  

Para fazer isso, vamos empregar tratamento de exceções. Ao executar o código, é possível perceber que apenas uma pequena quantidade de árvores de  parsing não pode ser normalizada.
"""

#desabilitando std_err. Para habilitar, basta chamar enable_stderr(r)
r=disable_stderr()

from nltk import treetransforms
from nltk import induce_pcfg
from nltk import Nonterminal

#contadores para árvores ok e para árvores com falha
ok=0;
falha=0;

psents=floresta.parsed_sents()
productions=[]

"""Processando as árvores de parsing com tratamento de exceção. É possível perceber que algumas das árvores não podem ser normalizadas para a forma normal de chomsky."""

for tree in psents:
  tree.collapse_unary(collapsePOS = False)# Transform branches A-B-C into A-(B+C)
  try:
    tree.chomsky_normal_form(horzMarkov = 2)# Transform A->(B,C,D) into A->B,(C+D)->D
    productions += tree.productions()     
    ok=ok+1
  except:
    print(tree,"\n")
    falha=falha+1

print("falha=", falha)
print("ok=", ok)

"""Inspecionando as 10 primeiras regras de produção normalizadas"""

productions[:10]

"""Inspecionando apenas as regras de produção começando pelo símbolo não terminal S."""

for p in productions:
  if p.lhs() == Nonterminal('S'):
    print(p)

"""Induzindo a gramática probabilística a partir das regras de produção normalizadas e imprimindo as regras de produção probabilísticas resultantes."""

S = Nonterminal('S')
grammar = induce_pcfg(S, productions)

for p in grammar.productions():
    print(p)

"""Acessando uma sentença do corpus 'floresta' para fins de teste."""

from nltk.parse import pchart
new_parser = pchart.InsideChartParser(grammar)
sent = floresta.parsed_sents()[23].leaves()
print(sent)

"""Ao tentarmos realizar o parsing da sentença acima, verificamos que há uma mensagem de erro indicando problema nas regras de produção. Conforme veremos mais abaixo, há regras na gramática probabilística em que o lado direito está vazio e o lado esquerdo contém caracteres especiais. Provavelmente, o corpus 'floresta' tem árvores inconsistentes que precisariam ser filtradas, mas esta tarefa está fora do escopo do que se pretendia com a Atividade 10. Desta forma, encerraremos neste ponto."""

new_parser.parse_all(sent)

"""Mostrando que existem regras de produção inválidas (lado direito sem símbolos), o que impede realizar o parsing da sentença fornecida."""

prod_str=[]
for prod in grammar.productions():
    if len(prod.rhs()) > 0 :
       prod_str.append(str(prod))
    else:
      print(prod)

"""## Exemplos de uso de uma palavra
Vamos criar uma função recebe uma palavra e um tamanho de contexto (em número de caracteres à esquerda e à direita),  e produz como resposta exemplos de uso da tal palavra (pode também ser útil para saber a concordância).
"""

def exemplos_uso(word, context=30):
    print(f"palavra: {word}, contexto: {context} caracteres")
    for sent in floresta.sents():
        if word in sent:
            pos = sent.index(word)
            left = " ".join(sent[:pos])
            right = " ".join(sent[pos + 1:])
            print(f"{left[-context:]} '{word}' {right[:context]}")

exemplos_uso("dar")

"""#Agora é a sua vez
Com base na função acima, crie uma nova função que receba uma lista de palavras (não existe mais a noção de contexto) e imprima exemplos de sentenças em que todas as palavras estejam presentes. Lembre-se de destacar com apróstofes, nos exemplos retornados, as palavras passadas como parâmetro.
Exemplo de chamada:

exemplos_uso_palavras(['começar','dar'])
"""

def exemplos_uso_palavras(lista):
    print(f"palavras: {lista}")
    for sent in floresta.sents():
  
      if set(lista).issubset(sent):
 
            for i in range(len(sent)):
              if sent[i] in lista:
                sent[i] = "'" + sent[i] + "'"

            a = juntar(sent)
            print(a)
            #pos = sent.index(word)
            #left = " ".join(sent[:pos])
            #right = " ".join(sent[pos + 1:])
            #print(f"{left} '{word}' {right}")


exemplos_uso_palavras(['começar','dar'])

def juntar(lista):
    if not lista:
        return ''
    if len(lista) == 1:
        return lista[0]

    s = ' '.join(lista[:-1])
    return f'{s} {lista[-1]}'